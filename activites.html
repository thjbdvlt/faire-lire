<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>visualisation de relations lexicales entre activités littéraires</title>
  <link rel="stylesheet" href="css/style.css" />
  <script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>
<p>On conçoit souvent la littérature comme un <em>corpus</em>. C’est la
définition qu’on trouve sur <a
href="https://fr.wikipedia.org/wiki/Littérature">Wikipedia</a>:</p>
<blockquote>
<p>La <strong>littérature</strong> est l’ensemble des oeuvres écrites ou
orales auxquelles on reconnaît une valeur esthétique</p>
</blockquote>
<p>On peut préférer la considérer comme une activité<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, ou
plus exactement, comme <q>un réseau d’activité<span class="citation"
data-cites="becker2013a"><a href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a></span></q>: <em>lire</em>,
<em>traduire</em>, <em>écrire</em>, mais aussi <em>acheter</em>,
<em>recommander</em>, <em>corriger</em>, <em>commenter</em>, etc.</p>
<p>Dans ce dépôt, je vais explorer ces activités, leurs proximités et
leurs relations sur la base de l’analyse lexicale d’un corpus de textes
que j’ai constitué et annoté dans le cadre de mon mémoire de master: les
fils de discussions publiquement accessibles du forum <a
href="https://www.jeunesecrivains.com/">Jeunes Écrivain·es</a>, un forum
d’entraide consacré à la littérature.</p>
<p>Le corpus est constitué de 133040 messages (<em>posts</em>) répartis
dans 6118 fils de discussions (<em>topics</em>). L’annotation a été
réalisée en utilisant <a href="https://spacy.io/">spaCy</a>, une
libraire (python) d’analyse du langage naturel (<em>NLP</em>).
<strong>spaCy</strong> propose des modèles d’analyses pour le français
mais ceux-ci n’étant pas adaptés à mon corpus (en fait, à mon avis,
assez peu adapté à n’importe quel corpus<a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>),
j’ai écris plusieurs modules (et entraîné quelques modèles) permettant
de réaliser les différentes tâches nécessaires à l’annotation de textes:
un <a
href="https://github.com/thjbdvlt/quelquhui"><em>tokenizer</em></a>, un
<a href="https://github.com/thjbdvlt/presque"><em>normalizer</em></a>,
un <a
href="https://github.com/thjbdvlt/turlututu"><em>morphologizer</em></a>,
un <a
href="https://github.com/thjbdvlt/viceverser"><em>lemmatizer</em></a> et
un <a
href="https://github.com/thjbdvlt/french-dependency-parser"><em>syntactic
dependency parser</em></a> qui reposent, pour certains sur des <a
href="https://github.com/thjbdvlt/french-word-vectors"><em>word
vectors</em></a>. Je ne rentre pas dans les détails de ces opérations,
qui sont décrites <a href="#annotation">plus bas</a>, au besoin.</p>
<p>Le corpus est stocké sous la forme d’une base de données <a
href="https://www.postgresql.org/">Postgresql</a> dans un schéma qui
implémente un modèle <a
href="https://en.wikipedia.org/wiki/Entity–attribute–value_model">EAV</a>
hybride et que j’ai conçu pour l’analyse de textes en français (le code
et la documentation du schéma sont disponibles <a
href="https://github.com/thjbdvlt/litteralement">ici</a>). Les données
utilisées dans les visualisations qui vont suivre ont (plus ou moins)
simplement été récupérées de cette base de données <em>via</em> des
courts scripts SQL disponibles dans le dossier <a
href="./sql">sql</a>.</p>
<h1 id="deux-tableaux">deux tableaux</h1>
<p>Quelles sont donc ces activités littéraires, et quelles sont leurs
relations?</p>
<p>Parmi les première activités littéraires auxquelles on est
susceptible de penser il y a, il me semble, <em>lire</em> et
<em>écrire</em>. Une première manière, très simple, d’explorer les
relations qu’ont d’autres activités avec celle-ci est de regarder les
cooccurrences les plus fréquentes. Les tableaux suivants montrent les
verbes qu’on retrouve le plus souvent dans une même phrase que les
verbes <em>lire</em> ou <em>écrire</em>. (Ce sont les lemmes qui sont
pris en compte: <q>je lirai ce que tu as écris</q> et <q>nous préférons
lire qu’écrire</q> sont donc équivalents.)</p>
<div id="cooccurrence">

</div>
<script src="./js/cooccurrence.js"></script>
<p>Les première lignes de chaque tableaux peuvent sembler assez
insignifiantes: le verbe le plus fréquemment employé dans une même
phrase que <em>lire</em> est… <em>lire</em> (<q>j’ai lu Wittgenstein
avant de lire Weil</q>); idem pour écrire. Le second est <em>écrire</em>
(<q>lire et écrire</q>). Les lignes qui suivent montrent des verbes qui
sont tout simplement très fréquents en français (dans le tableau, les
cooccurrences ne sont pas relatives mais absolues): <em>savoir</em>,
<em>aimer</em>, <em>mettre</em>, <em>prendre</em>, etc. Pourtant, il y a
des variations: <em>savoir</em>, <em>trouver</em>, <em>prendre</em> sont
aux mêmes positions pour <em>lire</em> et <em>écrire</em>, mais les
positions de <em>aimer</em> et <em>penser</em> sont inversées. On
<q>aime lire</q> et on <q>pense écrire</q>, moins l’inverse.</p>
<p>La position de <q>commencer</q> est aussi très différente dans les
deux tableaux: <q>commencer à écrire</q> est beaucoup plus fréquent que
<q>commencer à lire</q>. On peut, je crois, sans mal le comprendre, car
écrire un roman (la forme dominante sur le forum) prend bien davantage
de temps que le lire (plus un processus est long, et plus il apparait
naturel de dire qu’on l’a <q>commencé</q>: <q>commencé une série</q>
mais pas vraiment <q>commencé un morceau de musique</q>, etc.).
Certaines variations sont moins liées au sens d’un mot qu’à des
locutions particulières. Ainsi, <q>donner</q> est beaucoup plus fréquent
avec <q>lire</q> qu’avec <q>écrire</q> probablement en raison de
l’expression <q>donner à lire</q>.</p>
<p>Presque tout en bas de ces tableaux on trouve <em>publier</em>. C’est
le premier verbe qui semble vraiment en lien directe avec <em>lire</em>
et <em>écrire</em>. Sur le forum, il y a en effet beaucoup de
discussions consacrées à la publication (dont on imagine sans mal le
rapport à l’écriture). Mais il y en a encore beaucoup plus consacrées au
processus d’écriture lui-même, au sujet duquel on demande des conseils.
Or, <em>publier</em> a peu de synonyme (tout au plus <q>éditer</q>),
contrairement, par exemple, à <em>conseiller</em> (suggérer,
recommander, proposer, …). C’est là un des inconvénients à travailler
avec des simples cooccurrences. Un autre inconvénient, c’est
l’importance, évoquée plus haut, que prennent les verbes qui sont tout
simplement très fréquents (mais comme solution à cela, il y a la
pondération par la fréquence totale).</p>
<p>Une solution alternative qui ne pose pas ces mêmes problèmes, ou pas
tous (qui en pose bien sûr d’autres) et l’utilisation de <a
href="https://en.wikipedia.org/wiki/Word_embedding"><em>word
embeddings</em></a> ou <em>word vectors</em>. Les <em>word
embeddings</em> représentent les mots comme des points dans des espaces
euclidiens à <em>n</em> dimensions (ceux que j’ai entraînés ont 100
dimensions, d’autres en ont beaucoup plus). Plus un mot (un point) est
proche d’un autre et plus il est <strong>sémantiquement</strong> proche.
Ou plutôt, d’un point de vue algorithmique: plus il a tendance à être
utilisé dans des contextes similaires. Selon une conception figurative
(représentationnelle) du langage, cela n’a rien à voir avec <q>être
sémantiquement proche</q>. Mais si l’on adopte une conception
wittgensteinienne du langage, cette idée n’est pas sans fondement. Selon
Wittgenstein, la signification d’un mot est moins définie par une
<q>chose</q> à laquelle ce mot ferait référence ou à un <q>concept</q>
qu’il contiendrait qu’aux <q>jeux de langage<span class="citation"
data-cites="wittgenstein2014"><a href="#fn4" class="footnote-ref"
id="fnref4" role="doc-noteref"><sup>4</sup></a></span></q> qui le
mobilisent: <q>la signification d’un mot c’est l’usage de ce mot<span
class="citation" data-cites="wittgenstein2014"><a href="#fn5"
class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a></span></q>. La signification du mot
<q>bonjour</q> n’est pas autre chose que le genre de situation dans
lesquels on utilise ce mot (qui ne fait pas vraiment <q>référence</q> à
quoi que ce soit<span class="citation" data-cites="wittgenstein2014"><a
href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a></span>). <q>Vrai</q> et <q>faux</q>
sont sémantiquement très proches parce qu’on réalise avec eux le même
genre d’actions dans le même genre de situations. Wittgenstein dirait:
leurs <q>grammaires</q> sont similaires. Leurs <em>word embeddings</em>
seraient très proches.</p>
<p>Une représentation minimale d’une tout petit modèle de <em>word
embeddings</em> pourrait représenter à ceci:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode txt"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>lire    [ 0.3, -0.2,   -1,  0.1]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>écrire  [ 0.1, -0.4,    1,  -0.7]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>parler  [-0.4,  0.1, -0.4,    1]</span></code></pre></div>
<p>Les dimensions (les axes géométriques) ne correspondent pas à une
chose tangible, mais on pourrait se les représenter comme des axes
sémantiques ou d’usage: la première dimension pourrait être la dimension
<em>négative</em> pour <em>positive</em>, la deuxième dimension serait
la dimension <em>active</em> ou <em>passive</em> la troisième dimension
serait la dimension <em>familière</em> ou <em>soutenue</em>, etc. On
pourrait alors imaginer que les <em>word embeddings</em> de <q>vrai</q>
et de <q>faux</q> sont absolument identique à l’exception d’une seule
dimension (la dimension de <q>vérité</q>) pour laquelle <q>vrai</q>
aurait la valeur <code>1</code> et <q>faux</q> la valeur
<code>-1</code>. En réalité, il s’agit juste de dimensions <em>ad
hoc</em> initiées aléatoirement avec lesquelles construire un modèle
statistique, mais les percevoir ainsi nous permet de comprendre
certaines des choses qu’on peut faire avec. L’une des tâches
d’évaluations auxquelles les <em>word embeddings</em> sont parfois
soumis est dite <em>d’analogie</em>. Elle consiste à prendre un mot
<code>A1</code>, par exemple <q>France</q> auquel on soustrait un mot
<code>A2</code>, <q>Paris</q>. <q>Soustraire</q>, parce que l’avantage
réel des <em>word embeddings</em> est la possibilité de réaliser des
opérations mathématiques sur des mots. On soustrait donc <code>A2</code>
(<q>Paris</q>) à <code>A1</code> (<q>France</q>), et on obtient un
vecteur <code>v</code>. On additionne ensuite ce vecteur <code>v</code>
à un point <code>B1</code>, disons <q>Allemagne</q> et l’on doit pouvoir
prévoir le point <code>B2 = B1 + v</code>: <q>Berlin</q>. <q>Berlin</q>
(<code>B2</code>) est dans la même relation sémantique à
<q>Allemagne</q> (<code>B1</code>) que <q>Paris</q> (<code>A2</code>) à
<q>France</q> (<code>A1</code>) car le vecteur qui relie <q>Paris</q> à
<q>France</q> est (devrait être) identique au vecteur qui relie
<q>Berlin</q> à <q>Allemagne</q> (ce sont des vecteurs parallèles de
mêmes longueur): c’est le vecteur <q>être la capitale de</q>.</p>
<p>{…}</p>
<p>ce réseau d’activité est aussi, naturellement, un réseau de personnes
(ou plutôt d’acteur·rices). c’est, pour reprendre la terminologie du
sociologue Howard Becker, un <em>monde</em> (un <q>monde de l’art<span
class="citation" data-cites="becker2017"><a href="#fn7"
class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a></span></q>).</p>
<script src="./js/composition-roles.js"></script>
<h1 id="annotation">annotation</h1>
<p>L’annotation de texte avec <strong>spaCy</strong> se fait en
définissant une <em>pipeline</em>, une succession de fonction appliqué à
un <a href="https://spacy.io/api/doc">document</a>. Un document est une
liste de mots et des informations les concernant: leur nature
grammaticale (<em>part-of-speech</em> – par exemple <q>verbe</q>), leur
position dans le texte, leur noyau (<em>head</em>), leur fonction
(<em>dependency</em> – par exemple <q>sujet</q>), leur lemme
(<em>lemma</em> – le mot infléchi, par exemple <q>être</q> pour
<q>êtes</q>), etc.</p>
<ul class="incremental">
<li>Au tout début de cette <em>pipeline</em> se trouve une opération qui
peut sembler assez simple mais est en réalité assez délicate: la
<em>tokenization</em>, le découpage d’une chaîne de caractère (texte) en
<em>tokens</em> (mots, ponctuation, etc.). Si un texte comme <q>cette
partie est merveilleuse.</q> est facile à réaliser, il en va déjà
autrement pour un texte comme <q>les étudiant.e.s s’informent auprès de
A. pour le(s) dossier(s).</q>. Les modèles proposés par
<strong>spaCy</strong> ne parviennent pas à correctement découper un
texte comme celui-ci, parce que les parenthèses <q>intra-mots</q>
(<q>dossier(s)</q>) seront toujours séparées du mot lui-même, produisant
un <em>token</em> isolé (<q>s</q>) qui sera alors analysé comme un
pronom réflexif (<q>se</q>, <q>s’</q>). Par ailleurs, ils ne parviennent
pas à correctement découper les différentes formes d’écriture inclusive,
et ont un comportement aléatoire en ce qui concerne les traits d’unions.
J’ai donc écrit un <em>tokenizer</em>, <a
href="https://github.com/thjbdvlt/quelquhui">quelquhui</a>, pour
réaliser cette tâche.</li>
<li>Comme beaucoup de textes, les textes de mon corpus contiennent des
formes dysorthographiques (<q>vous etes</q>) et des variations
typographiques (<q>J’AAAARRIIIVVVE</q>). Pour pouvoir analyser
correctement le sens de ces expressions, j’ai écrit un
<em>normalizer</em>, <a
href="https://github.com/thjbdvlt/presque">presque</a> qui ramène ces
expressions vers leurs forme canoniques (<q>vous êtes</q>,
<q>j’arrive</q>). Cette opération ne modifie pas le texte, mais assigne
à valeur à la propriété <code>norm</code> de chaque <em>token</em>.
Toutes les opérations suivantes feront la même chose: assigner, pour
chaque <em>token</em> une valeur à une propriété.</li>
<li>Le <em>morphologizer</em> <a
href="https://github.com/thjbdvlt/turlututu">turlututu</a> assigne les
<em>part-of-speech</em> (<q>noun</q>, <q>verb</q>, <q>adj</q>, …) et les
caractéristiques <em>morphologies</em> (<q>Number</q>, <q>Tense</q>,
<q>Person</q>, etc.) au format FEATS. Il s’agit d’un modèle neuronal
entraîné sur un corpus que j’ai annoté de façon semi-automatique.</li>
<li>Le <em>lemmatizer</em> <a
href="https://github.com/thjbdvlt/viceverser">viceverser</a> détermine
les <em>lemmes</em>, les formes non-fléchies des mots (<q>parcourir</q>
pour <q>parcourions</q>) en utilisant un lexique au format <a
href="http://hunspell.github.io/">Hunspell</a> (un programme de
correction orthographique et de d’analyse morphologique) que j’ai
fabriqué à partir des fichiers du logiciel <a
href="https://grammalecte.net/">Grammalecte</a> et qui est disponible <a
href="https://github.com/thjbdvlt/spell-fr.vim">ici</a> (il s’agissait à
la base, pour moi, de réaliser un correcteur orthographique pour
l’éditeur de texte <a href="https://www.vim.org/">Vim</a> qui soit
décent pour le français).</li>
<li>Le <em>parser</em> <a
href="https://github.com/thjbdvlt/french-dependency-parser">french-dependency-parser</a>
effectue le <em>syntactic dependency parsing</em>: l’analyse des
relations syntaxiques entre les mots et assigne des valeurs à deux
propriétés: <code>head</code> (le noyau du mot) et <code>dep</code> (la
fonction syntaxique relative au noyau).</li>
</ul>
<p>Une partie de l’efficacité des modèles statistiques
(<em>morphologizer</em> et <em>parser</em>) repose sur l’usage de
<em>word vectors</em> de bonne qualité. Ceux que j’ai entraînés et qui
sont disponibles <a
href="https://github.com/thjbdvlt/french-word-vectors">ici</a> ont été
entraînés sur un corpus d’environ 500M de <em>tokens</em> que j’ai
obtenu par l’aggrégation de <em>corpora</em> linguistiques sous licence
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.fr">CC
BY-NC-SA</a> provenant notamment de <a
href="https://www.ortolang.fr/">Ortolang</a>, mais aussi de romans
(traductions de Jack London, par exemple), de textes de sciences
humaines dans le domaine public (Weil, Wittgenstein, Mauss, entre autres
choses) et de romans contemporains sous licence compatible avec la <a
href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.fr">CC
BY-NC-SA</a>. La lemmatisation et la normalisation du <em>corpus</em> a
été réalisé avec <strong>spaCy</strong> et l’entraînement avec <a
href="https://radimrehurek.com/gensim/">Gensim</a> avec l’algorithme <a
href="https://radimrehurek.com/gensim/models/word2vec.html">Word2Vec</a>
(CBOW).</p>
<h1 class="unnumbered" id="bibliographie">bibliographie</h1>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-becker2017" class="csl-entry" role="listitem">
<span class="smallcaps">Becker</span>, Howard Saul, <em>Les mondes de
l’art</em>, trad. Jeanne Bouniort, 3e éd., Paris, Flammarion, coll.
Champs, 2017.
</div>
<div id="ref-becker2013a" class="csl-entry" role="listitem">
─, <em>Propos sur l’art</em>, Paris, L’Harmattan, 2013.
</div>
<div id="ref-coste2017a" class="csl-entry" role="listitem">
<span class="smallcaps">Coste</span>, Florent, <em>Explore</em>, Paris,
Questions Théoriques, 2017.
</div>
<div id="ref-meizoz2016" class="csl-entry" role="listitem">
<span class="smallcaps">Meizoz</span>, Jérôme, <em>La littérature en
personne - scène médiatique et formes d’incarnation</em>, Genève,
Slatkine, 2016.
</div>
<div id="ref-wittgenstein2014" class="csl-entry" role="listitem">
<span class="smallcaps">Wittgenstein</span>, Ludwig, <em>Recherches
philosophiques</em>, trad. Françoise Dastur, Maurice Élie, Jean-Luc
Gautero, Dominique Janicaud et Élisabeth Rigal, Paris, Gallimard, coll.
tel, 2014.
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>voir, par exemple, les textes de Florent Coste<span
class="citation" data-cites="coste2017a"> (<span
class="smallcaps">Coste</span>, Florent, <em>Explore</em>, Paris,
Questions Théoriques, 2017)</span>, ou dans une certaine mesure de
Jérôme Meizoz <span class="citation" data-cites="meizoz2016"> (<span
class="smallcaps">Meizoz</span>, Jérôme, <em>La littérature en personne
- scène médiatique et formes d’incarnation</em>, Genève, Slatkine,
2016)</span>. le deuxième paragraphe de l’article Wikipedia
<q>Littérature</q> ajoute à la définition de la littérature comme corpus
la définition de la littérature comme activité (mais uniquement comme
activité de <em>création</em>, excluant donc par exemple la lecture); au
reste, cette définition n’est pas la définition que donne l’article,
mais uniquement une précision sur l’évolution de la notion de
littérature.<a href="#fnref1" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="smallcaps">Becker</span>, Howard Saul,
<em>Propos sur l’art</em>, Paris, L’Harmattan, 2013, 81. La citation de
Becker n’est pas spécifique à la littérature et concerne n’importe que
”monde de l’art”.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Les modèles proposés par <strong>spaCy</strong> pour le
français ont été entraînés sur des données extrêmement incomplètes. Un
mot, entre autre, y est par exemple totalement absent (et est toujours
systématiquement annoté de façon aléatoire): le pronom <em>tu</em> (le
corpus d’entraînement est uniquement constitué de textes issus de la
presse écrite).<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="smallcaps">Wittgenstein</span>, Ludwig,
<em>Recherches philosophiques</em>, trad. Françoise Dastur, Maurice
Élie, Jean-Luc Gautero, Dominique Janicaud et Élisabeth Rigal, Paris,
Gallimard, coll. tel, 2014.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><em>Ibid.</em><a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><em>Ibid.</em><a href="#fnref6" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="smallcaps">Becker</span>, Howard Saul,
<em>Les mondes de l’art</em>, trad. Jeanne Bouniort, 3e éd., Paris,
Flammarion, coll. Champs, 2017.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
